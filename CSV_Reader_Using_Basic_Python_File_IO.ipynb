{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243ccc1d",
   "metadata": {},
   "source": [
    "# Data Ingestion and Data Wrangling\n",
    "### Introduction\n",
    "This jupytehr notebook has two main tasks:\n",
    "- Data Ingestion. After having found the data, this is the second step in data science, where data are in practise moved from one place to another. In this notebook means, importing the data from cvs file format into python.\n",
    "- Data Wrangling. This might include several processes that are designed to transform raw data into usable data.  In this notebook data wrangling will be done by calcualting the the minimum, maximum, mean, and standard deviation of every single feature and identification of any possible outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801883fc",
   "metadata": {},
   "source": [
    "### Part 1: Data Ingestion\n",
    "\n",
    "#### Step 1: parsing the CSV files provided by the unit\n",
    "Parsing files included into the provided weather-dat.zip\n",
    "\n",
    "#### Step 2: parsing any CSV files\n",
    "Capable of parsing any other profided CSV files, considering both **input data** and **format variations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb8b7cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def data_ingestion(file):\n",
    "    \"\"\"Function which takes in input a string containing the input .csv file that needs to be parsed and ingested.\n",
    "    It is also capable to hanldle many different malformed data:\n",
    "    > missing elements in a cell\n",
    "    > not omogenous data type in a specific column\n",
    "    > date time format handling\n",
    "    > identify columns that are numerical hence can be usedd for statistical analysis\n",
    "    This function returns a tuple containing the information: \n",
    "    > if there is or not the column headers\n",
    "    > which column is made of int and float elements\n",
    "    > the actual data set.\"\"\"    \n",
    "    \n",
    "    # Open and read file in input\n",
    "    raw_list = []\n",
    "    with open(file, mode='r', encoding='utf-8-sig') as file: \n",
    "        for row in file:\n",
    "            # eliminate the space string and split based on comma\n",
    "            line = row.strip().split(\",\")\n",
    "            # append lines to create a final list\n",
    "            temp = []\n",
    "            temp.append(line[0])\n",
    "            for val in range(1,len(line)):\n",
    "                temp.append(line[val])\n",
    "            raw_list.append(temp)\n",
    "    \n",
    "    # Using python array instead of list\n",
    "    raw_array = np.array(raw_list)\n",
    "\n",
    "    # Find out if there is or not the column headers line\n",
    "    # If every element in the top row is not a digit then that row is the header columns, hence column_headers_present = 1\n",
    "    n_rows = len(raw_array[:, 0])\n",
    "    n_cols = len(raw_array[0, :])\n",
    "    flg_digit = np.empty(shape=(n_rows, n_cols), dtype=int)\n",
    "    is_header = np.empty(shape=(n_rows, 1), dtype=int)\n",
    "    for row in range(n_rows):\n",
    "        for col in range(n_cols):\n",
    "            flg_digit[row, col] = raw_array[row, col].isdigit()\n",
    "            is_header[row, 0] = not any(flg_digit[row, :]) and row == 0\n",
    "\n",
    "    column_headers_present = 0\n",
    "    # If there is column headers then split the data into header and values lists \n",
    "    if is_header[0, 0] == 1:\n",
    "        column_headers_present = 1\n",
    "        len_header = len(raw_array[0, :])\n",
    "        header = raw_array[0, :].reshape((1, len_header))\n",
    "        raw_values = raw_array[1:, :]\n",
    "        n_samples = n_rows - 1 # if there is the header columns then -1 to get the samples only\n",
    "    else:\n",
    "        raw_values = raw_array[:, :]\n",
    "        n_samples = n_rows\n",
    "\n",
    "    # Remove quotes (\"\") from the values. \n",
    "    values = np.empty(shape=(n_samples, n_cols), dtype=object)\n",
    "    for row in range(n_samples):\n",
    "        for col in range(n_cols):\n",
    "            values[row, col] = raw_values[row, col].replace('\"','')\n",
    "            \n",
    "    # identify empty cells, then delete the entire row that contains at least one empty cell\n",
    "    empty_cell = np.empty(shape=(n_samples, 1), dtype=object)\n",
    "    for row in range(n_samples):\n",
    "        for col in range(n_cols):\n",
    "            if values[row, col] == '':\n",
    "                empty_cell[row, 0] = 1\n",
    "  \n",
    "    raw_values = values.copy()\n",
    "    values = raw_values[empty_cell[:,0] != 1]\n",
    "    # recalculate n_samples since they might have been reduced\n",
    "    n_samples = len(values[:, 0])\n",
    "    if n_samples == 0:\n",
    "        print(\"Sorry, I can't progress it, there are zero valid samples available!\")\n",
    "        return\n",
    "\n",
    "    # Detect if a column contains date and time and which one is.\n",
    "    # Convert date and time into standard format.\n",
    "    raw_values = values.copy()\n",
    "    values = np.empty(shape=(n_samples, n_cols), dtype=object)\n",
    "    time_date_flg = np.empty(shape=(n_samples, n_cols), dtype=int)\n",
    "    for col in range(n_cols):\n",
    "        for row in range(n_samples):\n",
    "            try:  \n",
    "                # Convert date and time into standard date time format\n",
    "                values[row, col] = pd.to_datetime(raw_values[row, col])\n",
    "                # Set time_date_flg to 1 which means that the row contains date time field\n",
    "                time_date_flg[row, col] = 1\n",
    "            except:\n",
    "                # Set time_date_flg to 0 which means that the row does not contain date time field\n",
    "                time_date_flg[row, col] = 0\n",
    "                try:\n",
    "                    # If cannot convert date time it implies the column does not contain date time field\n",
    "                    values[row, col] = float(raw_values[row, col])\n",
    "                except:\n",
    "                    values[row, col] = raw_values[row, col]\n",
    "                    if column_headers_present == 1:\n",
    "                        row_new = row + 1\n",
    "                        col_new = col + 1\n",
    "                        print(f\"Could not convert row = {row_new} col ={col_new} to float\")\n",
    "                    else:\n",
    "                        row_new = row\n",
    "                        col_new = col\n",
    "                        print(f\"Could not convert row = {row_new} col ={col_new} to float\")                        \n",
    "                    \n",
    "    # find for every cell if it is numeric (integer or float) or not\n",
    "    is_int_float = np.empty(shape=(n_samples, n_cols), dtype=int)\n",
    "    for row in range(n_samples):\n",
    "        for col in range(n_cols):\n",
    "            if ( type(values[row, col]) == int ) or ( type(values[row, col]) == float ):\n",
    "                is_int_float[row, col] = 1\n",
    "            else:\n",
    "                is_int_float[row, col] = 0   \n",
    "\n",
    "    # find for every feature (column) if it is more frequent the numeric value (integer or float) or not\n",
    "    count_int_float = np.empty(shape=(1, n_cols), dtype=int)\n",
    "    common_int_float = np.empty(shape=(1, n_cols), dtype=int)\n",
    "    for col in range(n_cols):\n",
    "        # if = 1 it means that the most frequent type of value in the column is integer or floating\n",
    "        common_int_float[0, col] = np.argmax(np.bincount(is_int_float[:, col]))\n",
    "\n",
    "    # take out the rows that contains values with type different from the most common type in that column\n",
    "    wrong_type_cell = np.empty(shape=(n_samples, 1), dtype=object)\n",
    "    raw_values = values.copy()\n",
    "    for col in range(n_cols):\n",
    "        for row in range(n_samples):\n",
    "                # assign value 1 to the rows that have at least one cell which is not alligned with the rest of cells in their belong columns\n",
    "            if (common_int_float[0, col] == 1) and (type(raw_values[row, col]) != float) and (type(raw_values[row, col]) != int): \n",
    "                wrong_type_cell[row, 0] = 1  \n",
    "                \n",
    "    # keep only the rows that don't have a cell which is not alligned with the most common type of it belonging column\n",
    "    values = raw_values[wrong_type_cell[:,0] != 1]\n",
    "    # recalculating the number of sample in case one or more rows have been deleted\n",
    "    n_samples = len(values[:, 0])\n",
    "    if n_samples == 0:\n",
    "        print(\"Sorry, I can't progress it, there are zero valid samples available!\")\n",
    "        return\n",
    "    \n",
    "    counts_time_date_flg = np.empty(shape=(1, n_cols), dtype=int)\n",
    "    for col in range(n_cols):\n",
    "        counts_time_date_flg[0, col] = np.count_nonzero(time_date_flg[:, col] == 1)        \n",
    "\n",
    "    raw_values = values.copy()\n",
    "    for col in range(n_cols):\n",
    "        if counts_time_date_flg[0, col] < n_samples and counts_time_date_flg[0, col] > 0:\n",
    "            print(f\"Column '{header[0, col]}' is an incomplete date time column\")\n",
    "            \n",
    "    # malformed data. Header column should have the same number of fieds as the values\n",
    "    # malformed data. Every sample should have the same numer of fields\n",
    "    len_row_val_calc = np.empty(shape=(n_samples, 1), dtype=int)\n",
    "    for row in range(n_samples):\n",
    "        len_row_val_calc[row, 0] = len(values[row, :])\n",
    "\n",
    "    # length of every single row of the dataset\n",
    "    len_row_val = np.concatenate(len_row_val_calc)\n",
    "\n",
    "    # It gives back the most frequent number of fields in the data (length of the rows)\n",
    "    count_fields = np.bincount(len_row_val)  \n",
    "    common_fields_n = np.argmax(count_fields)\n",
    "\n",
    "    # Check if the header columns has the right number of fields compare to the rest of the data\n",
    "    if column_headers_present == 1 and len_header != common_fields_n:\n",
    "        print(\"The columns header has different number of field from the values!\")\n",
    "    # Check if every single sample has the right number of fields compare to the rest of the values\n",
    "    for row in range(n_samples):\n",
    "        if len_row_val_calc[row, 0] != common_fields_n:\n",
    "            print(f\"Row {row} has different number of fields respect to the rest of values, please correct it!\")\n",
    "    \n",
    "    # if there is a column header then concatenate it to the rest of the values\n",
    "    if column_headers_present == 1:\n",
    "        data = np.concatenate((header, values), axis=0)\n",
    "    else:\n",
    "        data = values\n",
    "    return column_headers_present, common_int_float, data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62a857",
   "metadata": {},
   "source": [
    "For the **USER**, please define the name of the csv file that you want to process 'file_name':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32421f8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Input the file name to be processed\n",
    "file_name = \"barometer-1617.csv\" \n",
    "\n",
    "data_ingest = data_ingestion(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e86bb7",
   "metadata": {},
   "source": [
    "### Part 2: Data Wrangling\n",
    "\n",
    "#### Step 1: minimum, maximum, mean, and standard deviation\n",
    "Computing minimum, maximum, mean, and standard deviation for file weather-dat.zip\n",
    "\n",
    "#### Step 2: with outliers: minimum, maximum, mean, and standard deviation\n",
    "Computing minimum, maximum, mean, and standard deviation for a new file that contains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa17863",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def data_wrangling(input_data):\n",
    "    \"\"\"Function which takes in input a tuple containing: the information if there is or not the column headers and the data set. \n",
    "    It returns a dataframe containing the minimum, maximum, mean and standard deviation of every single column (feature).\"\"\"\n",
    "    \n",
    "    column_headers_present = input_data[0]\n",
    "    common_int_float = input_data[1]\n",
    "    data = input_data[2]\n",
    "    #print(\"data\", data)\n",
    "\n",
    "    n_rows = len(data[:, 0])\n",
    "    n_cols = len(data[0, :])\n",
    "    if column_headers_present == 1:\n",
    "        raw_header = data[0, :]\n",
    "        raw_values = data[1:, :]\n",
    "        n_samples = n_rows - 1\n",
    "    else:\n",
    "        raw_values = data[:, :]\n",
    "        n_samples = n_rows\n",
    "    \n",
    "    # keep only the header columns and values that are integer and float\n",
    "    header = raw_header[common_int_float[0, :] == 1]\n",
    "    n_cols_stats = len(header[:])\n",
    "    values = np.empty(shape=(n_samples, n_cols_stats), dtype=object)\n",
    "    for row in range(n_samples):\n",
    "        values[row, :] = raw_values[row, :][common_int_float[0, :] == 1]\n",
    "    \n",
    "    # Calculate the basic statistics: minimum, maximum, mean and standard deviation for every feature (column)\n",
    "    min_val = np.empty(shape=(1, n_cols_stats), dtype=object)\n",
    "    max_val = np.empty(shape=(1, n_cols_stats), dtype=object)\n",
    "    mean_val = np.empty(shape=(1, n_cols_stats), dtype=object)\n",
    "    std_val = np.empty(shape=(1, n_cols_stats), dtype=object)\n",
    "    for col in range(n_cols_stats):\n",
    "        min_val[0, col] = np.min(values[:, col])\n",
    "        max_val[0, col] = np.max(values[:, col])\n",
    "        mean_val[0, col] = np.mean(values[:, col])\n",
    "        std_val[0, col] = np.std(values[:, col])\n",
    "    \n",
    "    statistics_names = [\"min\", \"max\", \"mean\", \"stand. dev.\"]\n",
    "    statistics_data = np.concatenate((min_val, max_val, mean_val, std_val), axis=0)\n",
    "\n",
    "    # Create a data frame to better handle the col and row labels for showing the basic statistics\n",
    "    data_frame_statist = pd.DataFrame(statistics_data, statistics_names, header)\n",
    "    \n",
    "    return data_frame_statist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a331b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name: barometer-1617.csv\n",
      "\n",
      "                  \"Baro\"\n",
      "min                979.6\n",
      "max               1035.6\n",
      "mean         1009.998873\n",
      "stand. dev.     9.855751\n"
     ]
    }
   ],
   "source": [
    "# Using the ingested data from the previous phase to apply data wrangling\n",
    "print(\"file name:\", file_name)\n",
    "print()\n",
    "if data_ingest is None:\n",
    "    print (\"No statistics available because 'data_ingest' is empty\")\n",
    "\n",
    "else:\n",
    "    data_wrang = data_wrangling(data_ingest)\n",
    "    print(data_wrang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
